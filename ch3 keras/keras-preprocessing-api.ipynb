{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Date | User | Change Type | Remarks |  \n",
    "| ---- | ---- | ----------- | ------- |\n",
    "| 11/12/2024   | Martin | Created   | Started Keras Preprcocessing API | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Content\n",
    "\n",
    "* [Sequence Preprocessing](#sequence-preprocessing)\n",
    "* [Text Preprocessing](#text-preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Preprocessing API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data processing and data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator, pad_sequences, skipgrams, make_sampling_table\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence, hashing_trick, Tokenizer\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GRU\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"GRPC_VERBOSITY\"] = \"ERROR\"\n",
    "os.environ[\"GLOG_minloglevel\"] = \"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence preprocessing\n",
    "\n",
    "_Sequence_ data is where older data matter like text or time series data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series generator\n",
    "\n",
    "`TimeseriesGenerator` takes consecutive data points andapplies transformations using time series parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: [0 1 2 3 4 5 6 7 8 9]\n",
      "Samples: 5\n",
      "[[0 1 2 3 4]] => [5]\n",
      "[[1 2 3 4 5]] => [6]\n",
      "[[2 3 4 5 6]] => [7]\n",
      "[[3 4 5 6 7]] => [8]\n",
      "[[4 5 6 7 8]] => [9]\n"
     ]
    }
   ],
   "source": [
    "series = np.array([i for i in range(10)])\n",
    "print(f'Original data: {series}')\n",
    "\n",
    "# predict the next value based on the last 5 lagging observations\n",
    "generator = TimeseriesGenerator(\n",
    "  data=series,\n",
    "  targets=series,\n",
    "  length=5,\n",
    "  batch_size=1,\n",
    "  shuffle=False,\n",
    "  reverse=False\n",
    ")\n",
    "print(f\"Samples: {len(generator)}\")\n",
    "\n",
    "for i in range(len(generator)):\n",
    "  x, y = generator[i]\n",
    "  print(f\"{x} => {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 60.4673  \n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 52.3983 \n",
      "Epoch 3/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 49.2140 \n",
      "Epoch 4/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 51.8469 \n",
      "Epoch 5/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 51.6252 \n",
      "Epoch 6/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 50.5210 \n",
      "Epoch 7/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 44.9877 \n",
      "Epoch 8/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 37.1479 \n",
      "Epoch 9/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 42.7858 \n",
      "Epoch 10/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 41.5767 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f93f81e9290>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', input_dim=5))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train model\n",
    "model.fit(\n",
    "  generator,\n",
    "  epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding sequences\n",
    "\n",
    "Sequence data often have different lengths that need to be processed to fit the same dimensions.\n",
    "Padding is to increase the length of shorter sequences to match the larger ones.\n",
    "\n",
    "For time series data padding is usually done at the beginning of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [[\"What\", \"do\", \"you\", \"like\", \"?\"],\n",
    "             [\"I\", \"like\", \"basket-ball\", \"!\"],\n",
    "             [\"And\", \"you\", \"?\"],\n",
    "             [\"I\", \"like\", \"coconut\", \"and\", \"apple\"]]\n",
    "\n",
    "# Build the vocabulary\n",
    "text_set = set(np.concatenate(sentences))\n",
    "vocab_to_int = dict(zip(text_set, range(len(text_set))))\n",
    "int_to_vocab = {vocab_to_int[word]:word for word in vocab_to_int.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8, 1, 6, 0, 10], [4, 0, 7, 5], [3, 6, 10], [4, 0, 9, 11, 2]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 8,  1,  6,  0, 10],\n",
       "       [ 0,  4,  0,  7,  5],\n",
       "       [ 0,  0,  3,  6, 10],\n",
       "       [ 4,  0,  9, 11,  2]], dtype=int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the sentences\n",
    "encoded_sentences = []\n",
    "for sentence in sentences:\n",
    "  encoded_sentence = [vocab_to_int[word] for word in sentence]\n",
    "  encoded_sentences.append(encoded_sentence)\n",
    "\n",
    "# Pad the shorter ones\n",
    "print(encoded_sentences)\n",
    "pad_sequences(encoded_sentences)\n",
    "# maxlen, truncating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-grams\n",
    "\n",
    "Unsupervised learning techniques in NLP - finds the most related words for a given word and predicts the context of the given word.\n",
    "\n",
    "`skipgrams` in Tensorflow takes in a integer-encoded pair of words and returns their relevance (1 if relevant 0 otherwise). A context word is selected which all examples are compared against, then a window is selected to determine the number of comparisons to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(apple -> apple) -> 1\n",
      "(and -> and) -> 1\n",
      "(coconut -> coconut) -> 1\n",
      "(and -> and) -> 1\n"
     ]
    }
   ],
   "source": [
    "# Encode sentence into integers\n",
    "sentence = \"I like coconut and apple\"\n",
    "encoded_sentence = [vocab_to_int[word] for word in sentence.split()]\n",
    "vocabulary_size = len(encoded_sentence)\n",
    "\n",
    "# Setup skipgram\n",
    "pairs, labels = skipgrams(\n",
    "  encoded_sentence,\n",
    "  vocabulary_size,\n",
    "  window_size=1,\n",
    "  negative_samples=0\n",
    ")\n",
    "\n",
    "# Print the relevancy\n",
    "for i in range(len(pairs)):\n",
    "  print(f\"({int_to_vocab[pairs[i][0]]} -> {int_to_vocab[pairs[i][0]]}) -> {labels[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to encode text as numbers and provide integers as inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split text to word sequence\n",
    "\n",
    "`text_to_word_sequence` - Transforms a sequence into a list of words/ tokens. Able to set to lowercsae and remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'like', 'coconut', 'I', 'like', 'apple']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"I like coconut, I like apple\"\n",
    "text_to_word_sequence(sentence, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'like', 'coconut,', 'i', 'like', 'apple']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_word_sequence(sentence, lower=True, filters=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokeniser\n",
    "\n",
    "`Tokenizer` - converts strings/ paragraphs into individual tokens based on the configuration specified\n",
    "\n",
    "Inputs:\n",
    "\n",
    "* Max number of words to keep, based on frequency\n",
    "* List of characters to filter out\n",
    "* Boolean to convert lower case or not\n",
    "* Separator for word splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "  [\"What\", \"do\", \"you\", \"like\", \"?\"],\n",
    "  [\"I\", \"like\", \"basket-ball\", \"!\"],\n",
    "  [\"And\", \"you\", \"?\"],\n",
    "  [\"I\", \"like\", \"coconut\", \"and\", \"apple\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tokenizer\n",
    "t = Tokenizer()\n",
    "\n",
    "# fit tokenizer on documents\n",
    "t.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('what', 1), ('do', 1), ('you', 2), ('like', 3), ('?', 2), ('i', 2), ('basket-ball', 1), ('!', 1), ('and', 2), ('coconut', 1), ('apple', 1)])\n",
      "4\n",
      "{'like': 1, 'you': 2, '?': 3, 'i': 4, 'and': 5, 'what': 6, 'do': 7, 'basket-ball': 8, '!': 9, 'coconut': 10, 'apple': 11}\n",
      "defaultdict(<class 'int'>, {'?': 2, 'do': 1, 'what': 1, 'like': 3, 'you': 2, 'basket-ball': 1, 'i': 2, '!': 1, 'and': 2, 'apple': 1, 'coconut': 1})\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer contains useful information in metadata\n",
    "## Count of each word across all documents\n",
    "print(t.word_counts)\n",
    "\n",
    "## Number of documents\n",
    "print(t.document_count)\n",
    "\n",
    "## Unique index identifier\n",
    "print(t.word_index)\n",
    "\n",
    "## Number of documents (in this case lists) that each word appears in\n",
    "print(t.word_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg 157"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Date | User | Change Type | Remarks |  \n",
    "| ---- | ---- | ----------- | ------- |\n",
    "| 13/03/2025   | Martin | Created   | Created notebook for reinforcement learning design patterns. | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Content\n",
    "\n",
    "* [Introduction](#introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TF-Agents` is the reinforcement learning (RL) library for Tensorflow. \n",
    "\n",
    "__Components__\n",
    "\n",
    "* Agent operates in an __environement__ and learns by processing signals received every time it chooses an action. Environment is implemented in Python and wrapped in TF wrapper for efficient parallelization\n",
    "* __Policy__ maps an observation from the environment into a distribution over some actions\n",
    "* __Driver__ executes the policy in an environment for a specified number of steps (_episodes_)\n",
    "* __Replay Buffer__ stores the experience (agent trajectories in action space and associated rewards) of executing a policy in an environment; buffer content is queried for a subset of trajectories during training\n",
    "\n",
    "ðŸ’¡ __IDEA:__ Cast each problem as a RL problem and map parameters into the corresponding TF-Agent component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The Game:__ 6x6 square board, where the agent starts at (0,0), the finish is at (5,5), and the goal of the agent is to find the path from the start to the finish\n",
    "\n",
    "__Actions:__ Possible actions are moves up/down/left/right\n",
    "\n",
    "__Rewards:__  If the agent lands on the finish, it receives a reward of 100, and the game terminates after 100 steps if the end was not reached by the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 18:35:27.039426: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-13 18:35:27.049718: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741862127.061314   10569 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741862127.065020   10569 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-13 18:35:27.077978: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents.environments import py_environment, \\\n",
    "                                   tf_environment, \\\n",
    "                                   tf_py_environment, \\\n",
    "                                   utils, \\\n",
    "                                   wrappers, \\\n",
    "                                   suite_gym\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.trajectories import trajectory, time_step as ts\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.metrics import tf_metrics, py_metrics\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.drivers import py_driver, dynamic_episode_driver\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the environment the agents will operate in. Define the conditions for resetting/ terminating the state, and mechanics for moving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldEnv(py_environment.PyEnvironment):\n",
    "  def __init__(self):\n",
    "    # BoundedArraySpec defines the min and max values that an ArraySpec can take\n",
    "    # ArraySpec is a condition of what values can be inside the array\n",
    "    self._action_spec = array_spec.BoundedArraySpec(\n",
    "      shape=(),\n",
    "      dtype=np.int32,\n",
    "      minimum=0,\n",
    "      maximum=3,\n",
    "      name='action'\n",
    "    ) # Actions that were taken\n",
    "    self._observation_spec = array_spec.BoundedArraySpec(\n",
    "      shape=(4, ),\n",
    "      dtype=np.int32,\n",
    "      minimum=[0, 0, 0, 0],\n",
    "      maximum=[5, 5, 5, 5],\n",
    "      name='observation'\n",
    "    ) # Set of observations i.e results for each epoch\n",
    "    self._state = [0, 0, 5, 5] # Represent the (row, col, frow, fcol) of the player and the finish\n",
    "    self._episode_ended = False\n",
    "  \n",
    "  def action_spec(self):\n",
    "    return self._action_spec\n",
    "  \n",
    "  def observation_spec(self):\n",
    "    return self._observation_spec\n",
    "\n",
    "  def _reset(self):\n",
    "    \"\"\"\n",
    "    Reset to the next episode\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    self._state = [0, 0, 5, 5]\n",
    "    self._episode_ended = False\n",
    "    return ts.restart(np.array(self._state, dtype=np.int32))\n",
    "  \n",
    "  def _step(self, action):\n",
    "    \"\"\"\n",
    "    Each step:\n",
    "      1. Check if the episode has ended\n",
    "        - if yes, reset episode\n",
    "        - if no, continue to move using current action\n",
    "      2. Check if there is a game over \n",
    "        - if yes, end the episode\n",
    "        - return corresponding reward\n",
    "\n",
    "    Args:\n",
    "        action (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    if self._episode_ended:\n",
    "      return self.reset()\n",
    "    \n",
    "    self.move(action)\n",
    "\n",
    "    if self.game_over():\n",
    "      self._episode_ended = True\n",
    "    \n",
    "    if self._episode_ended:\n",
    "      if self.game_over():\n",
    "        reward = 100\n",
    "      else:\n",
    "        reward = 0\n",
    "      return ts.termination(np.array(self._state, dtype=np.int32), reward) # End of game\n",
    "    else:\n",
    "      return ts.transition(np.array(self._state, dtype=np.int32), reward=0, discount=0.9) # Transition to next step of game\n",
    "    \n",
    "  def move(self, action):\n",
    "    row, col, frow, fcol = self._state[0], self._state[1], self._state[2], self._state[3],\n",
    "    if action == 0: #down\n",
    "      if row - 1 >= 0:\n",
    "        self._state[0] -= 1\n",
    "    if action == 1: #up\n",
    "      if row + 1 < 6:\n",
    "        self._state[0] += 1\n",
    "    if action == 2: #left\n",
    "      if col - 1 >= 0:\n",
    "        self._state[1] -= 1\n",
    "    if action == 3: #right\n",
    "      if col + 1  < 6:\n",
    "        self._state[1] += 1\n",
    "  \n",
    "  def game_over(self):\n",
    "    row, col, frow, fcol = self._state[0], self._state[1], self._state[2], self._state[3],\n",
    "    return row == frow and col == fcol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_late():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "      total_return += episode_return\n",
    "  \n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: 2025-03-13T18:14:08.783191+08:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.10.12\n",
      "IPython version      : 8.33.0\n",
      "\n",
      "Compiler    : GCC 11.4.0\n",
      "OS          : Linux\n",
      "Release     : 5.15.167.4-microsoft-standard-WSL2\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 20\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Date | User | Change Type | Remarks |  \n",
    "| ---- | ---- | ----------- | ------- |\n",
    "| 03/03/2025   | Martin | Created   | Created notebook for Transformers Chapter. Started text generation section and exploring different decoding methods | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Content\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [Text Generation](#text-generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers perform a similar function to RNNs in processing sequential data but are an improvement since they do not require processing of data in order. Results in better parallelisation and faster training.\n",
    "\n",
    "They can be pretrained on large bodies of unlabeled data and then fintuned for other tasks\n",
    "\n",
    "__Perform Functions__\n",
    "\n",
    "* Translation\n",
    "* Question answering\n",
    "* Text summarisations\n",
    "\n",
    "__2 Common Architectures__\n",
    "\n",
    "1. Bidirectional Encoder Represetations for Transformers (BERT)\n",
    "2. Generative Pretrained Transformers (GPTs)\n",
    "\n",
    "__Recipes Covered__\n",
    "\n",
    "1. Text generation\n",
    "2. Sentiment Analysis\n",
    "3. Text classification: sarcasm detection\n",
    "4. Question answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using GPT-2 from __HuggingFace__\n",
    "\n",
    "_GPT-2:_ Second generation of the GPT architecture. It showed show generative language models can acquire knowledge and process long-range dependencies thanks to pretraining on a large, diverse corpus of contiguous text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "GPT2 = TFGPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "SEED = 34\n",
    "MAX_LEN = 70 # set the maximum context length\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding Methods\n",
    "\n",
    "These are different methods on how to determine the next word\n",
    "\n",
    "1. __Greedy Search__ - Select the word with the highest probability\n",
    "2. [__Beam Search__](https://towardsdatascience.com/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24/) - Picks the N best sequences and considers the probabilities of the combination of all the preceding words + current word. Each branching sequence is retrained against the model conditioned on all previously selected words till the end of a sentence (\"\") has the highest probability\n",
    "3. __Sample-based decoding__ - Randomly selecting the next token according to a probability distribution. The distribution is created based on the probabilities for each word, which are conditional probabilities\n",
    "4. __Top K Sampling__ - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Greedy Search\n",
    "\n",
    "Model tends to repeat itself after awhile, because the high probability words mask the less probable ones which prevents any exploration for diverse combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample texts/ sequences\n",
    "input_sequence_1 = \"I don't know about you, but there's only one thing I want to do after a long day of work\"\n",
    "input_sequence_2 = \"There are times when I am really tired of people, but I feel lonely too.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "There are times when I am really tired of people, but I feel lonely too. I feel like I am not in the right place at the right time. I feel like I am not in the right place at the right time. I feel like I am not in the right place at the right time. I feel like I am not in the\n"
     ]
    }
   ],
   "source": [
    "# 1. Greedy Search\n",
    "# Encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode(input_sequence_2, return_tensors='tf')\n",
    "\n",
    "# Generate text until the output length (including context length - input)\n",
    "greedy_output = GPT2.generate(input_ids, max_length=MAX_LEN)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Beam Search\n",
    "\n",
    "Selects the N best sequences by selecting the best N combinations and creating branching possibilities of next words. Each branch is retrained with the previously selected sequence to get a new set of possible word probabilities.\n",
    "\n",
    "e.g For the sentence: \"I am going to the ____\"\n",
    "\n",
    "Possible predictions:\n",
    "\n",
    "* Park 0.6\n",
    "* Zoo 0.1\n",
    "* Doctor 0.25\n",
    "* Supermarket 0.05\n",
    "\n",
    "With N=2, the beam search will rerun the model on \"I am going to the __Park__\" and \"I am going to the __Supermarket__\" and then repeat the process for each result\n",
    "\n",
    "Each iteration will select the highest probability across __ALL__ branches. This method increases the exploration of possible words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1: There are times when I am really tired of people, but I feel lonely too. I don't know what to do with myself.\n",
      "\n",
      "\"I feel like I can't do anything about it. It's like, 'Oh my God, I'm going to have to get out of here.'\"\n",
      "2: There are times when I am really tired of people, but I feel lonely too. I don't know what to do with myself.\n",
      "\n",
      "\"I feel like I can't do anything about it. It's like, 'Oh my God, I'm going to have to get out of here.' I just want to go home.\"\n",
      "3: There are times when I am really tired of people, but I feel lonely too. I don't know what to do with myself.\n",
      "\n",
      "\"I feel like I can't do anything about it. It's like, 'Oh my God, I'm going to have to get out of here.' I have no idea what's going on.\"\n",
      "4: There are times when I am really tired of people, but I feel lonely too. I don't know what to do with myself.\n",
      "\n",
      "\"I feel like I can't do anything about it. It's like, 'Oh my God, I'm going to have to get out of here.' I have no idea what's going on, so I just keep going.\"\n",
      "5: There are times when I am really tired of people, but I feel lonely too. I don't know what to do with myself.\n",
      "\n",
      "\"I feel like I can't do anything about it. It's like, 'Oh my God, I'm going to have to get out of here.' I have no idea what's going on, so I just go to sleep.\"\n"
     ]
    }
   ],
   "source": [
    "# 2. Beam Search\n",
    "input_ids = tokenizer.encode(input_sequence_2, return_tensors='tf')\n",
    "\n",
    "beam_outputs = GPT2.generate(\n",
    "  input_ids,\n",
    "  max_length=200,\n",
    "  num_beams=5,\n",
    "  no_repeat_ngram_size=2,\n",
    "  num_return_sequences=5,\n",
    "  early_stopping=True\n",
    ")\n",
    "\n",
    "print('')\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "\n",
    "# we have 5 different outputs\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(f\"{i+1}: {tokenizer.decode(beam_output, skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sample-based Decoding\n",
    "\n",
    "Randomly selecting the next token according to its probability distribution. \n",
    "\n",
    "Convert the scores into probability distributions (conditional distributions) and randomly select a token\n",
    "\n",
    "_Temperature:_ Controls the \"sharpness\" of the probability distribution\n",
    "\n",
    "* Low Temperature (0.1-0.5): Makes high-probability more likely to be chosen (Leptokurtic) i.e More focused and predictable text\n",
    "* High Temperature (0.8-1.5): Flattens the distribution and gives more equal chance to lower probability tokens (Platykurtic). Increases diversity, bit risks incoherence\n",
    "\n",
    "__OUTCOME:__ Have a more varied output, but sometimes is incoherent esp. with a higher temperature value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I don't know about you, but there's only one thing I want to do after a long day of work. I want to go to the gym and get my body ready for the next day. I want to go to the gym and get my body ready for the next day. I want to go to the gym and get my body ready\n"
     ]
    }
   ],
   "source": [
    "# 3. Sample-based decoding\n",
    "input_ids = tokenizer.encode(input_sequence_1, return_tensors='tf')\n",
    "\n",
    "sample_output = GPT2.generate(\n",
    "  input_ids,\n",
    "  do_sample=True,\n",
    "  max_length=MAX_LEN,\n",
    "  top_k=0,\n",
    "  temperature=0.2 # relatively low temperature value to maintain coherence\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I don't know about you, but there's only one thing I want to do after a long day of work. This is the day I leave the house,\" she said.\n",
      "\n",
      "\"You have to be ready for something,\" Zhaoxiu said. \"I am afraid to leave.\"\n",
      "\n",
      "She didn't leave alone.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with a higher temperature value to increase variability in tokens\n",
    "sample_output = GPT2.generate(\n",
    "  input_ids,\n",
    "  do_sample=True,\n",
    "  max_length=MAX_LEN,\n",
    "  top_k=0,\n",
    "  temperature=0.8 # relatively low temperature value to maintain coherence\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Date | User | Change Type | Remarks |  \n",
    "| ---- | ---- | ----------- | ------- |\n",
    "| 03/03/2025   | Martin | Created   | Created notebook for Transformers Chapter. Started text generation section and exploring different decoding methods | \n",
    "| 04/03/2025   | Martin | Update   | Completed top k and top p methods for decoding section | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Content\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [Text Generation](#text-generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers perform a similar function to RNNs in processing sequential data but are an improvement since they do not require processing of data in order. Results in better parallelisation and faster training.\n",
    "\n",
    "They can be pretrained on large bodies of unlabeled data and then fintuned for other tasks\n",
    "\n",
    "__Perform Functions__\n",
    "\n",
    "* Translation\n",
    "* Question answering\n",
    "* Text summarisations\n",
    "\n",
    "__2 Common Architectures__\n",
    "\n",
    "1. Bidirectional Encoder Represetations for Transformers (BERT)\n",
    "2. Generative Pretrained Transformers (GPTs)\n",
    "\n",
    "__Recipes Covered__\n",
    "\n",
    "1. Text generation\n",
    "2. Sentiment Analysis\n",
    "3. Text classification: sarcasm detection\n",
    "4. Question answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using GPT-2 from __HuggingFace__\n",
    "\n",
    "_GPT-2:_ Second generation of the GPT architecture. It showed show generative language models can acquire knowledge and process long-range dependencies thanks to pretraining on a large, diverse corpus of contiguous text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 23:12:20.017064: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-04 23:12:22.302923: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741101143.101419    1444 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741101143.337128    1444 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-04 23:12:25.479633: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/mnt/d/Software/venv/py310_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "I0000 00:00:1741101202.346323    1444 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9558 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "GPT2 = TFGPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "SEED = 34\n",
    "MAX_LEN = 70 # set the maximum context length\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding Methods\n",
    "\n",
    "These are different methods on how to determine the next word\n",
    "\n",
    "1. __Greedy Search__ - Select the word with the highest probability\n",
    "2. [__Beam Search__](https://towardsdatascience.com/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24/) - Picks the N best sequences and considers the probabilities of the combination of all the preceding words + current word. Each branching sequence is retrained against the model conditioned on all previously selected words till the end of a sentence (\"\") has the highest probability\n",
    "3. __Sample-based decoding__ - Randomly selecting the next token according to a probability distribution. The distribution is created based on the probabilities for each word, which are conditional probabilities\n",
    "4. __Top K Sampling__ - Entire probability mass is shifted to only the top $k$ words. This increases the chances of high probability words occurring and decreases the chances of low probability words\n",
    "5. __Top P Sampling (Nucleus Sampling)__ - Choose the smallest set of words whose total probability is greater than $p$. The probability mass function is rescaled to these set of words. This means that the size of the set of words changes with each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Greedy Search\n",
    "\n",
    "Model tends to repeat itself after awhile, because the high probability words mask the less probable ones which prevents any exploration for diverse combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample texts/ sequences\n",
    "input_sequence_1 = \"I don't know about you, but there's only one thing I want to do after a long day of work\"\n",
    "input_sequence_2 = \"There are times when I am really tired of people, but I feel lonely too.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "There are times when I am really tired of people, but I feel lonely too. I feel like I am not in the right place at the right time. I feel like I am not in the right place at the right time. I feel like I am not in the right place at the right time. I feel like I am not in the\n"
     ]
    }
   ],
   "source": [
    "# 1. Greedy Search\n",
    "# Encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode(input_sequence_2, return_tensors='tf')\n",
    "\n",
    "# Generate text until the output length (including context length - input)\n",
    "greedy_output = GPT2.generate(input_ids, max_length=MAX_LEN)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Beam Search\n",
    "\n",
    "Selects the N best sequences by selecting the best N combinations and creating branching possibilities of next words. Each branch is retrained with the previously selected sequence to get a new set of possible word probabilities.\n",
    "\n",
    "e.g For the sentence: \"I am going to the ____\"\n",
    "\n",
    "Possible predictions:\n",
    "\n",
    "* Park 0.6\n",
    "* Zoo 0.1\n",
    "* Doctor 0.25\n",
    "* Supermarket 0.05\n",
    "\n",
    "With N=2, the beam search will rerun the model on \"I am going to the __Park__\" and \"I am going to the __Supermarket__\" and then repeat the process for each result\n",
    "\n",
    "Each iteration will select the highest probability across __ALL__ branches. This method increases the exploration of possible words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1: There are times when I am really tired of people, but I feel lonely too. I don't know what to do with myself.\n",
      "\n",
      "\"I feel like I can't do anything about it. It's like, 'Oh my God, I'm going to have to get out of here.'\"\n",
      "2: There are times when I am really tired of people, but I feel lonely too. I don't know what to do with myself.\n",
      "\n",
      "\"I feel like I can't do anything about it. It's like, 'Oh my God, I'm going to have to get out of here.' I just want to go home.\"\n",
      "3: There are times when I am really tired of people, but I feel lonely too. I don't know what to do with myself.\n",
      "\n",
      "\"I feel like I can't do anything about it. It's like, 'Oh my God, I'm going to have to get out of here.' I have no idea what's going on.\"\n",
      "4: There are times when I am really tired of people, but I feel lonely too. I don't know what to do with myself.\n",
      "\n",
      "\"I feel like I can't do anything about it. It's like, 'Oh my God, I'm going to have to get out of here.' I have no idea what's going on, so I just keep going.\"\n",
      "5: There are times when I am really tired of people, but I feel lonely too. I don't know what to do with myself.\n",
      "\n",
      "\"I feel like I can't do anything about it. It's like, 'Oh my God, I'm going to have to get out of here.' I have no idea what's going on, so I just go to sleep.\"\n"
     ]
    }
   ],
   "source": [
    "# 2. Beam Search\n",
    "input_ids = tokenizer.encode(input_sequence_2, return_tensors='tf')\n",
    "\n",
    "beam_outputs = GPT2.generate(\n",
    "  input_ids,\n",
    "  max_length=200,\n",
    "  num_beams=5,\n",
    "  no_repeat_ngram_size=2,\n",
    "  num_return_sequences=5,\n",
    "  early_stopping=True\n",
    ")\n",
    "\n",
    "print('')\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "\n",
    "# we have 5 different outputs\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(f\"{i+1}: {tokenizer.decode(beam_output, skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sample-based Decoding\n",
    "\n",
    "Randomly selecting the next token according to its probability distribution. \n",
    "\n",
    "Convert the scores into probability distributions (conditional distributions) and randomly select a token\n",
    "\n",
    "_Temperature:_ Controls the \"sharpness\" of the probability distribution\n",
    "\n",
    "* Low Temperature (0.1-0.5): Makes high-probability more likely to be chosen (Leptokurtic) i.e More focused and predictable text\n",
    "* High Temperature (0.8-1.5): Flattens the distribution and gives more equal chance to lower probability tokens (Platykurtic). Increases diversity, bit risks incoherence\n",
    "\n",
    "__OUTCOME:__ Have a more varied output, but sometimes is incoherent esp. with a higher temperature value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I don't know about you, but there's only one thing I want to do after a long day of work. I want to go to the gym and get my body ready for the next day. I want to go to the gym and get my body ready for the next day. I want to go to the gym and get my body ready\n"
     ]
    }
   ],
   "source": [
    "# 3. Sample-based decoding\n",
    "input_ids = tokenizer.encode(input_sequence_1, return_tensors='tf')\n",
    "\n",
    "sample_output = GPT2.generate(\n",
    "  input_ids,\n",
    "  do_sample=True,\n",
    "  max_length=MAX_LEN,\n",
    "  top_k=0,\n",
    "  temperature=0.2 # relatively low temperature value to maintain coherence\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I don't know about you, but there's only one thing I want to do after a long day of work. This is the day I leave the house,\" she said.\n",
      "\n",
      "\"You have to be ready for something,\" Zhaoxiu said. \"I am afraid to leave.\"\n",
      "\n",
      "She didn't leave alone.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with a higher temperature value to increase variability in tokens\n",
    "sample_output = GPT2.generate(\n",
    "  input_ids,\n",
    "  do_sample=True,\n",
    "  max_length=MAX_LEN,\n",
    "  top_k=0,\n",
    "  temperature=0.8 # relatively low temperature value to maintain coherence\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Top K Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I don't know about you, but there's only one thing I want to do after a long day of work. I have a great time. And that day I'm going to stop getting sick. Because you can't sit up with yourself and eat nothing, you have to sit up with yourself.\"\n",
      "\n",
      "If you are on social media, ...\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(input_sequence_1, return_tensors='tf')\n",
    "\n",
    "sample_output = GPT2.generate(\n",
    "  input_ids,\n",
    "  do_sample=True,\n",
    "  max_length=MAX_LEN,\n",
    "  top_k=50\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True), '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Top P Sample/ Nucleus Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I don't know about you, but there's only one thing I want to do after a long day of work that I can do. I want to get back to work, and get back to helping the community.\"\n",
      "\n",
      "But on a personal note, Zuckerman says he hopes his decision will provide him with hope for his daughter.\n",
      " ...\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(input_sequence_1, return_tensors='tf')\n",
    "\n",
    "sample_output = GPT2.generate(\n",
    "  input_ids,\n",
    "  do_sample=True,\n",
    "  max_length=MAX_LEN,\n",
    "  top_k=0,\n",
    "  top_p=0.8\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True), '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining both approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1: I don't know about you, but there's only one thing I want to do after a long day of work. I want to make my wife happy.\"\n",
      "\n",
      "When the day started, the young woman told me she felt sorry for her daughter, who was suffering from an autism spectrum disorder. She said that after working long hours, she felt like a burden. She had no idea that she had to take a stand on behalf of other children.\n",
      "\n",
      "\"I want my kids to learn how to be successful,\" the woman said. \"I want them to be able to do everything in their power to be successful, and to do that because they know what it's like to have no\n",
      "\n",
      "2: I don't know about you, but there's only one thing I want to do after a long day of work. I want to do this for you, for your children and for your grandchildren. I'm going to make sure you get a good education.\"\n",
      "\n",
      "A family member of mine, an Australian citizen, went on to write a long story in the Sunday Independent. In it she writes: \"I want to thank you for coming and I can't do it. I'm afraid of what's happening to me. You're a very sweet young woman and I'd like to know how you're going to deal with it. You've worked so hard for so long.\n",
      "\n",
      "\"\n",
      "\n",
      "3: I don't know about you, but there's only one thing I want to do after a long day of work and play the game of football. It's about winning and it's about being a good player, making the right decisions, going in and out of the box and I'm hoping that's what you'll get,\" said the 29-year-old striker.\n",
      "\n",
      "\"I feel really good about my career, and I feel like I've been able to improve my game a lot since I've been here.\n",
      "\n",
      "\"It was something that I wanted to do, and I've been getting a lot of support, but there's only one thing I want to do after a\n",
      "\n",
      "4: I don't know about you, but there's only one thing I want to do after a long day of work on this one: I want to be a better coach than I was a year ago.\"\n",
      "\n",
      "The Knicks' defense has been very good and the offense hasn't been good in any of the previous three seasons. Now, they have to put up points if they want to make the playoffs and then get back on track after a big win.\n",
      "\n",
      "\"We've been playing really well this year and we're going to do all the things that we can to be better,\" said Kristaps Porzingis, who is averaging 12.5 points and 8.8 rebounds this season\n",
      "\n",
      "5: I don't know about you, but there's only one thing I want to do after a long day of work on the project. I want to create something that will last a lifetime.\"\n",
      "\n",
      "That's what he has been working on, in the last five months.\n",
      "\n",
      "\"I'm going to do it,\" he says. \"You can see I'm not going to do it.\"\n",
      "\n",
      "What would it be like to take the project to the moon?\n",
      "\n",
      "\"It's not that hard,\" he says. \"I'm a big fan of the Moon landing. It's kind of a cool concept. But we need to take a deep breath. We don't want to\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(input_sequence_1, return_tensors='tf')\n",
    "\n",
    "sample_outputs = GPT2.generate(\n",
    "  input_ids,\n",
    "  do_sample=True,\n",
    "  max_length=2*MAX_LEN,\n",
    "  top_k=50,\n",
    "  top_p=0.8,\n",
    "  num_return_sequences=5\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(f\"{i+1}: {tokenizer.decode(sample_output, skip_special_tokens=True)}\")\n",
    "  print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

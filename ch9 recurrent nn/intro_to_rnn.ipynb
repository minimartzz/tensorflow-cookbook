{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Date | User | Change Type | Remarks |  \n",
    "| ---- | ---- | ----------- | ------- |\n",
    "| 28/02/25  | Martin | Created   | Create for chapter 9. Started on text generation section | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Content\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [Text Generation - LSTM](#text-generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Recurrent Neural Networks (RNN)__ model data that is sequential in nature. Recurrent refers to data where the output of the current step becomes the input to the next one. At each step, the model considers what it has seen about the preceding elements on top of the current input.\n",
    "\n",
    "__Natural Language Processing (NLP)__ is where we train models to understand text information by training them on the context\n",
    "\n",
    "Topics covered:\n",
    "\n",
    "1. Text generation\n",
    "2. Sentiment classification\n",
    "3. Time series - stock information\n",
    "4. Open-domain question answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a _Long Short-Term Memory (LSTM)_ architecture to build a text generation model\n",
    "\n",
    "* Standard RNN models suffer from long dependencies i.e words that are earlier in the context window no longer contribute to the model since they're further away (vanishing gradient problem).\n",
    "* LSTM maintains a cell state, and a \"carry\" to ensure the signal is not loss as the sequence progresses\n",
    "* Each step: (1) current word (2) carry (3) cell state\n",
    "\n",
    "Video References\n",
    "\n",
    "* [RNN Explained](https://www.youtube.com/watch?v=AsNTP8Kwu80&ab_channel=StatQuestwithJoshStarmer)\n",
    "* [LSTM Explained](https://www.youtube.com/watch?v=YCzL96nL7j0&ab_channel=StatQuestwithJoshStarmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "# Keras modules for LSTM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Emdedding, LSTM, Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(7)\n",
    "tf.random.set_seed(7)\n",
    "import random\n",
    "random.seed(7)\n",
    "tf.random.unifrom([1], seed=1)\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"GRPC_VERBOSITY\"] = \"ERROR\"\n",
    "os.environ[\"GLOG_minloglevel\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions that simplify the workflow\n",
    "def clean_text(txt):\n",
    "  \"\"\"\n",
    "  Removes punctuations and lowercase text.\n",
    "  Then convert text into utf-8 format\n",
    "  \"\"\"\n",
    "  txt = \"\".join(v for v in text if v not in string.punctuation).lower()\n",
    "  txt = txt.encode('utf8').decode('ascii', 'ignore')\n",
    "  return txt\n",
    "\n",
    "\n",
    "def get_sequence_of_tokens(corpus):\n",
    "  \"\"\"\n",
    "\n",
    "  \"\"\"\n",
    "  # Tokeniser\n",
    "  tokenizer.fit_on_text(corpus)\n",
    "  total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "  # Convert data to sequence of tokens\n",
    "  input_sequence = []\n",
    "  for line in corpus:\n",
    "    token_list = tokenizer.text_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "      n_gram_sequence = token_list[:i+1]\n",
    "      input_sequences.append(n_gram_sequence)\n",
    "  return input_sequence, total_words\n",
    "\n",
    "\n",
    "def generate_padded_sequences(input_sentences):\n",
    "  \"\"\"\n",
    "  1. Ensure that all the sequenes are of the same length by adding padding\n",
    "  All padding is added to the front\n",
    "  2. Separate the predictions (text content) and labels (last word in the sequence)\n",
    "  3. Convert the label into a categorical variable. Categories are all available words\n",
    "  in the corpus\n",
    "  \"\"\"\n",
    "  max_sequence_len = max([len(x) for x in input_sentences])\n",
    "  input_sequences = np.array(pad_sequences(\n",
    "    input_sequences,\n",
    "    maxlen=max_sequence_len,\n",
    "    padding='pre'\n",
    "  ))\n",
    "\n",
    "  predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "  label = ku.to_categorical(label, num_classes=total_words)\n",
    "  return predictors, label, max_sequence_len\n",
    "\n",
    "\n",
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "  \"\"\"\n",
    "  1. Add the same preprocessing done to the text\n",
    "  2. Make predictions of next word\n",
    "  3. Add the predicted word to the end of the seed text\n",
    "  \"\"\"\n",
    "  for _ in range(next_words):\n",
    "    # Apply the same preprocessing as the model\n",
    "    token_list = tokenizer.text_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences(\n",
    "      [token_list],\n",
    "      maxlen=max_sequence_len-1, # need to remove the label\n",
    "      padding='pre'\n",
    "    )\n",
    "\n",
    "    # Make a prediction on the next word\n",
    "    predicted = model.predict_classes(token_list, verbose=0)\n",
    "\n",
    "    # Convert the prediction back to actual word\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "      if index == predicted:\n",
    "        output_word = word\n",
    "        break\n",
    "      \n",
    "    seed_text += \" \" + output_word\n",
    "  \n",
    "  return seed_text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_sequence_len, total_words):\n",
    "  \"\"\"\n",
    "  Create model with single LSTM hidden layer\n",
    "  \"\"\"\n",
    "  input = max_sequence_len - 1\n",
    "  model = Sequential()\n",
    "\n",
    "  # Embedding layer\n",
    "  model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "\n",
    "  # LSTM layer\n",
    "  model.add(LSTM(100))\n",
    "\n",
    "  # Output layer\n",
    "  model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "  return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Date | User | Change Type | Remarks |  \n",
    "| ---- | ---- | ----------- | ------- |\n",
    "| 28/02/25  | Martin | Created   | Create for chapter 9. Started on text generation section | \n",
    "| 02/03/25  | Martin | Update   | Completed text generation with LSTM section | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Content\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [Text Generation - LSTM](#text-generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Recurrent Neural Networks (RNN)__ model data that is sequential in nature. Recurrent refers to data where the output of the current step becomes the input to the next one. At each step, the model considers what it has seen about the preceding elements on top of the current input.\n",
    "\n",
    "__Natural Language Processing (NLP)__ is where we train models to understand text information by training them on the context\n",
    "\n",
    "Topics covered:\n",
    "\n",
    "1. Text generation\n",
    "2. Sentiment classification\n",
    "3. Time series - stock information\n",
    "4. Open-domain question answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a _Long Short-Term Memory (LSTM)_ architecture to build a text generation model\n",
    "\n",
    "* Standard RNN models suffer from long dependencies i.e words that are earlier in the context window no longer contribute to the model since they're further away (vanishing gradient problem).\n",
    "* LSTM maintains a cell state, and a \"carry\" to ensure the signal is not loss as the sequence progresses\n",
    "* Each step: (1) current word (2) carry (3) cell state\n",
    "\n",
    "Video References\n",
    "\n",
    "* [RNN Explained](https://www.youtube.com/watch?v=AsNTP8Kwu80&ab_channel=StatQuestwithJoshStarmer)\n",
    "* [LSTM Explained](https://www.youtube.com/watch?v=YCzL96nL7j0&ab_channel=StatQuestwithJoshStarmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM Architecture](./images/lstm_architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1740926555.117657      18 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "# Keras modules for LSTM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(7)\n",
    "tf.random.set_seed(7)\n",
    "import random\n",
    "random.seed(7)\n",
    "tf.random.uniform([1], seed=1)\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"GRPC_VERBOSITY\"] = \"ERROR\"\n",
    "os.environ[\"GLOG_minloglevel\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions that simplify the workflow\n",
    "def clean_text(txt):\n",
    "  \"\"\"\n",
    "  Removes punctuations and lowercase text.\n",
    "  Then convert text into utf-8 format\n",
    "  \"\"\"\n",
    "  txt = \"\".join(v for v in text if v not in string.punctuation).lower()\n",
    "  txt = txt.encode('utf8').decode('ascii', 'ignore')\n",
    "  return txt\n",
    "\n",
    "\n",
    "def get_sequence_of_tokens(corpus):\n",
    "  \"\"\"\n",
    "  Creates an ngram sequence - a list of lists that contain the tokenised sentences.\n",
    "  For each sentence, everytime a new word is added to the list, it is added as a new indexed list\n",
    "  \"\"\"\n",
    "  # Tokeniser\n",
    "  tokenizer.fit_on_text(corpus)\n",
    "  total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "  # Convert data to sequence of tokens\n",
    "  input_sequence = []\n",
    "  for line in corpus:\n",
    "    token_list = tokenizer.text_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "      n_gram_sequence = token_list[:i+1]\n",
    "      input_sequences.append(n_gram_sequence)\n",
    "  return input_sequence, total_words\n",
    "\n",
    "\n",
    "def generate_padded_sequences(input_sentences):\n",
    "  \"\"\"\n",
    "  1. Ensure that all the sequenes are of the same length by adding padding\n",
    "  All padding is added to the front\n",
    "  2. Separate the predictions (text content) and labels (last word in the sequence)\n",
    "  3. Convert the label into a categorical variable. Categories are all available words\n",
    "  in the corpus\n",
    "  \"\"\"\n",
    "  max_sequence_len = max([len(x) for x in input_sentences])\n",
    "  input_sequences = np.array(pad_sequences(\n",
    "    input_sequences,\n",
    "    maxlen=max_sequence_len,\n",
    "    padding='pre'\n",
    "  ))\n",
    "\n",
    "  predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "  label = ku.to_categorical(label, num_classes=total_words)\n",
    "  return predictors, label, max_sequence_len\n",
    "\n",
    "\n",
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "  \"\"\"\n",
    "  1. Add the same preprocessing done to the text\n",
    "  2. Make predictions of next word\n",
    "  3. Add the predicted word to the end of the seed text\n",
    "  \"\"\"\n",
    "  for _ in range(next_words):\n",
    "    # Apply the same preprocessing as the model\n",
    "    token_list = tokenizer.text_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences(\n",
    "      [token_list],\n",
    "      maxlen=max_sequence_len-1, # need to remove the label\n",
    "      padding='pre'\n",
    "    )\n",
    "\n",
    "    # Make a prediction on the next word\n",
    "    predicted = model.predict_classes(token_list, verbose=0)\n",
    "\n",
    "    # Convert the prediction back to actual word\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "      if index == predicted:\n",
    "        output_word = word\n",
    "        break\n",
    "      \n",
    "    seed_text += \" \" + output_word\n",
    "  \n",
    "  return seed_text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_sequence_len, total_words):\n",
    "  \"\"\"\n",
    "  Create model with single LSTM hidden layer\n",
    "  \"\"\"\n",
    "  input = max_sequence_len - 1\n",
    "  model = Sequential()\n",
    "\n",
    "  # Embedding layer\n",
    "  model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "\n",
    "  # LSTM layer\n",
    "  model.add(LSTM(100))\n",
    "\n",
    "  # Output layer\n",
    "  model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"../data/ny_articles/\"\n",
    "all_headlines = []\n",
    "for f in os.listdir(directory):\n",
    "  article_df = pd.read_csv(directory + f)\n",
    "  all_headlines.extend(list(article_df.headline.values))\n",
    "\n",
    "all_headlines[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform data preparation steps:\n",
    "\n",
    "1. Remove punctuations and lower casing of all words\n",
    "2. _Tokenisation:_ Converting text into ngram sequences - _ngrams_ are lists of integers that encode the word from a standard corpus based on the index\n",
    "3. _Padding:_ Ensures that all the sequences are of the same length\n",
    "4. Create the predictors and labels: labels are just the next word in the sequence\n",
    "\n",
    "ðŸ“œ __NOTE:__ Language modeling requires a sequence input data, as given a squence (of words/ tokens) the aim is the prediction of the next word/ token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Removing punctuations, lower casing\n",
    "corpus = [clean_text(x) for x in all_headlines]\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Tokenisation for ngram sequences\n",
    "tokeniser = Tokenizer()\n",
    "\n",
    "inp_sequences, total_words = get_sequences_of_tokens(corpus)\n",
    "inp_sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 + 4. Padding sequences + create labels\n",
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(max_sequence_len, total_words)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(predictors, label, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (generate_text(\"united states\", 5, model, max_sequence_len))\n",
    "print (generate_text(\"united states\", 10, model, max_sequence_len))\n",
    "print (generate_text(\"united states\", 15, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (generate_text(\"president trump\", 3, model, max_sequence_len))\n",
    "print (generate_text(\"president trump\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"president trump\", 5, model, max_sequence_len))\n",
    "print (generate_text(\"president trump\", 8, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (generate_text(\"joe biden\", 3, model, max_sequence_len))\n",
    "print (generate_text(\"joe biden\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"joe biden\", 5, model, max_sequence_len))\n",
    "print (generate_text(\"joe biden\", 8, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (generate_text(\"india and china\", 3, model, max_sequence_len))\n",
    "print (generate_text(\"india and china\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"india and china\", 5, model, max_sequence_len))\n",
    "print (generate_text(\"india and china\", 8, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (generate_text(\"european union\", 3, model, max_sequence_len))\n",
    "print (generate_text(\"european union\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"european union\", 5, model, max_sequence_len))\n",
    "print (generate_text(\"european union\", 8, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

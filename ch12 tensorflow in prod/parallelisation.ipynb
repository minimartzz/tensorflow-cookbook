{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow in Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Date | User | Change Type | Remarks |  \n",
    "| ---- | ---- | ----------- | ------- |\n",
    "| 14/03/2025   | Martin | Created   | Created notebook for parallelisation snippets | \n",
    "| 16/03/2025   | Martin | Updated   | Completed parallelisation and device assignment | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Content\n",
    "\n",
    "* [Using Multiple Executors](#using-multiple-executors)\n",
    "* [Parallelising TensorFlow](#parallelising-tensorflow)\n",
    "* [Saving and Restoring TensorFlow Models](#saving-and-restoring-tensorflow-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Multiple Executors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computational graphs in TensorFlow are naturally meant to be computed in parallel. Computational graphs can be split over different processors and processed in different batches\n",
    "\n",
    "_How to access different processors on the same machine?_\n",
    "\n",
    "TF will automatically distribute the computation across the multiple devices via a greedy process, but you can also specify which operation should be performed on each device via a _name scope placement_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 18:16:23.942536: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-16 18:16:26.467238: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742120187.534811    2045 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742120187.803671    2045 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-16 18:16:30.249533: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs available, 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Num GPUs available, {len(tf.config.list_physical_devices('GPU'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a Tensorflow operation is implemented for CPU and GPU devices, the oepration will be executed by default on the GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "input: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "_EagerConst: (_EagerConst): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "input: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "_EagerConst: (_EagerConst): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "tensor: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "shape: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "a: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "b: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "product_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742120386.023824    2045 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9558 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2025-03-16 18:19:46.058010: I tensorflow/core/common_runtime/placer.cc:162] input: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2025-03-16 18:19:46.058039: I tensorflow/core/common_runtime/placer.cc:162] _EagerConst: (_EagerConst): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2025-03-16 18:19:46.058043: I tensorflow/core/common_runtime/placer.cc:162] output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2025-03-16 18:19:49.739507: I tensorflow/core/common_runtime/placer.cc:162] input: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "2025-03-16 18:19:49.739535: I tensorflow/core/common_runtime/placer.cc:162] _EagerConst: (_EagerConst): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2025-03-16 18:19:49.739538: I tensorflow/core/common_runtime/placer.cc:162] output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2025-03-16 18:19:49.758157: I tensorflow/core/common_runtime/placer.cc:162] tensor: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2025-03-16 18:19:49.758189: I tensorflow/core/common_runtime/placer.cc:162] shape: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "2025-03-16 18:19:49.758196: I tensorflow/core/common_runtime/placer.cc:162] Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2025-03-16 18:19:49.758199: I tensorflow/core/common_runtime/placer.cc:162] output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2025-03-16 18:19:49.766800: I tensorflow/core/common_runtime/placer.cc:162] a: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2025-03-16 18:19:49.766834: I tensorflow/core/common_runtime/placer.cc:162] b: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2025-03-16 18:19:49.766841: I tensorflow/core/common_runtime/placer.cc:162] MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2025-03-16 18:19:49.766844: I tensorflow/core/common_runtime/placer.cc:162] product_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# To find out where placement occurs, set 'log_device_placement'\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "c = tf.matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "/job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "/job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Or use `device` attribute to determine the name of device that tensor is on\n",
    "a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "print(a.device)\n",
    "b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "print(b.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the device\n",
    "\n",
    "Select the device to use by creating a device context with `tf.device` function. Each operation executed in the context will use the selected device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "shape: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Reshape: (Reshape): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "a: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "b: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "MatMul: (MatMul): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "product_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "a: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "b: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "_MklMatMul: (_MklMatMul): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "product_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op _MklMatMul in device /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 18:22:15.574271: I tensorflow/core/common_runtime/placer.cc:162] tensor: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "2025-03-16 18:22:15.574303: I tensorflow/core/common_runtime/placer.cc:162] shape: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "2025-03-16 18:22:15.574310: I tensorflow/core/common_runtime/placer.cc:162] Reshape: (Reshape): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "2025-03-16 18:22:15.574312: I tensorflow/core/common_runtime/placer.cc:162] output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "2025-03-16 18:22:15.588232: I tensorflow/core/common_runtime/placer.cc:162] a: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "2025-03-16 18:22:15.588259: I tensorflow/core/common_runtime/placer.cc:162] b: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "2025-03-16 18:22:15.588264: I tensorflow/core/common_runtime/placer.cc:162] MatMul: (MatMul): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "2025-03-16 18:22:15.588266: I tensorflow/core/common_runtime/placer.cc:162] product_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "2025-03-16 18:22:15.590837: I tensorflow/core/common_runtime/placer.cc:162] a: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "2025-03-16 18:22:15.590904: I tensorflow/core/common_runtime/placer.cc:162] b: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "2025-03-16 18:22:15.590912: I tensorflow/core/common_runtime/placer.cc:162] _MklMatMul: (_MklMatMul): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "2025-03-16 18:22:15.590915: I tensorflow/core/common_runtime/placer.cc:162] product_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "# Specifies that all operations should be run on the CPU\n",
    "with tf.device('/device:CPU:0'):\n",
    "  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "  c = tf.matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "# Specifies that only instantiate the variables on the CPU, but operation is carried out on GPU\n",
    "with tf.device('/device:CPU:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "c = tf.matmul(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limiting GPU memory allocation\n",
    "\n",
    "Tensorflow never releases GPU memory allocation. Starts with almost all of the GPU memory allocated\n",
    "\n",
    "Slowly grow to that limit with `tf.config.experimental.set_memory_grow` method option or set the environmental variable `TF_FORCE_GPU_ALLOW_GROWTH` to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical devices cannot be modified after being initialized\n"
     ]
    }
   ],
   "source": [
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth cannot be modified after GPU has been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using multiple GPUs\n",
    "\n",
    "Set the placements on multiple devices. Assuming there are 3 devices CPU:0, GPU:0 and GPU:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virtual devices cannot be modified after being initialized\n",
      "Num GPUs Available:  1\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "x: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "y: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Mul: (Mul): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "z_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "tf.Tensor([  88.  264.  440.  176.  528.  880.  264.  792. 1320.], shape=(9,), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 18:30:33.518109: I tensorflow/core/common_runtime/placer.cc:162] x: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "2025-03-16 18:30:33.518134: I tensorflow/core/common_runtime/placer.cc:162] y: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "2025-03-16 18:30:33.518140: I tensorflow/core/common_runtime/placer.cc:162] Mul: (Mul): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "2025-03-16 18:30:33.518142: I tensorflow/core/common_runtime/placer.cc:162] z_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "# Create 2 virtual GPUs\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "if gpu_devices:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "      gpu_devices[0],\n",
    "      # Create 2 virtual GPUs\n",
    "      [\n",
    "        tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024),\n",
    "        tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)\n",
    "      ]\n",
    "    )\n",
    "  except RuntimeError as e:\n",
    "    # Memory growht cannot be modified after GPU has been initialised\n",
    "    print(e)\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_logical_devices('GPU')))\n",
    "\n",
    "# Built-in function here to test if a GPU is available\n",
    "if tf.test.is_built_with_cuda():\n",
    "  with tf.device('/cpu:0'):\n",
    "    a = tf.constant([1.0, 3.0, 5.0], shape=[1, 3])\n",
    "    b = tf.constant([2.0, 4.0, 6.0], shape=[3, 1])\n",
    "\n",
    "    with tf.device('/gpu:0'):\n",
    "      c = tf.matmul(a, b)\n",
    "      c = tf.reshape(c, [-1])\n",
    "\n",
    "    with tf.device('/gpu:1'):\n",
    "      d = tf.matmul(b, a)\n",
    "      flat_d = tf.reshape(d, [-1])\n",
    "\n",
    "    combined = tf.multiply(c, flat_d)\n",
    "  \n",
    "  print(combined)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naming conventions\n",
    "\n",
    "Devices used by Tensorflow have different naming conventions\n",
    "\n",
    "| Device | Device Name |\n",
    "| ---- | ---- |\n",
    "| Main CPU   | `/device:CPU:0` |\n",
    "| Main GPU   | `/GPU:0` |\n",
    "| Second GPU   | `/job:localhost/replica:0/task:0/device:GPU:1` |\n",
    "| Third GPU   | `/job:localhost/replica:0/task:0/device:GPU:2` |\n",
    "\n",
    "CPUs are also considered as a unique processer. All cores are wrapped into the same CPU device i.e Tensorflow uses multiple CPU cores by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelising TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution strategy to speed up the training.\n",
    "\n",
    "Tensorflow distributed API is used to distribute the training by replicating the model into different nodes and training on different subsets of data.\n",
    "\n",
    "__Features__\n",
    "\n",
    "* Supports a hardware platform\n",
    "* Either synchronous or asynchronous training strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two virtual GPUs\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(gpu_devices[0],\n",
    "                                                [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024),\n",
    "                                                tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024) ])\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth cannot be modified after GPU has been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets, info = tfds.load('mnist', with_info=True, as_supervised=True)\n",
    "mnist_train, mnist_test = datasets['train'], datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "mnist_train = mnist_train.map(\n",
    "  normalize_img,\n",
    "  num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "mnist_train = mnist_train.cache()\n",
    "mnist_train = mnist_train.shuffle(info.splits['train'].num_examples)\n",
    "mnist_train = mnist_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "mnist_test = mnist_test.map(\n",
    "  normalize_img,\n",
    "  num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "mnist_test = mnist_test.cache()\n",
    "mnist_test = mnist_test.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying strategy\n",
    "\n",
    "Replicate the model across all GPUs on the same machine. Each model is trained on different batches of data and synchronous training strategy is applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "resource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "VarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "resource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "value: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 18:45:01.810928: I tensorflow/core/common_runtime/placer.cc:162] resource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2025-03-16 18:45:01.810963: I tensorflow/core/common_runtime/placer.cc:162] VarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2025-03-16 18:45:01.816902: I tensorflow/core/common_runtime/placer.cc:162] resource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2025-03-16 18:45:01.816928: I tensorflow/core/common_runtime/placer.cc:162] value: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2025-03-16 18:45:01.816935: I tensorflow/core/common_runtime/placer.cc:162] AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "mirrored_strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "print('Number of devices: {}'.format(mirrored_strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_PER_REPLICA = 128 # Each GPU receives 128 datapoints\n",
    "BATCH_SIZE = BATCH_SIZE_PER_REPLICA * mirrored_strategy.num_replicas_in_sync # Total batch size will its multiple\n",
    "\n",
    "mnist_train = mnist_train.batch(BATCH_SIZE)\n",
    "mnist_test = mnist_test.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define and compile model using the mirrored strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All vairables created inside the scope are mirrored across ALL replicas\n",
    "with mirrored_strategy.scope():\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(tf.keras.layers.Flatten(name=\"FLATTEN\"))\n",
    "  model.add(tf.keras.layers.Dense(units=128 , activation=\"relu\", name=\"D1\"))\n",
    "  model.add(tf.keras.layers.Dense(units=64 , activation=\"relu\", name=\"D2\"))\n",
    "  model.add(tf.keras.layers.Dense(units=10, activation=\"softmax\", name=\"OUTPUT\"))\n",
    "  \n",
    "  model.compile(\n",
    "    optimizer=\"sgd\", \n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "  mnist_train,\n",
    "  epochs=10,\n",
    "  validation_data=mnist_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed strategies\n",
    "\n",
    "* `TPUStrategy()` - TPU strategy is like mirrored strategy but runs on TPUs\n",
    "* `MultiWorkerMirroredStrategy()` - Multiworker Mirrored strategy is also similar to mirrored strategy but the model is trained across several machines, each with multiple GPUs. Specify the cross-device communication\n",
    "* `experimental.CentralStorageStrategy()` - Central Storage strategy uses a synchronous mode on one machine with multiple GPUs. Variables are not mirroed but placed on the CPU and operations are replicated to all local GPUs\n",
    "* `experimental.ParameterServerStrategy()` - Parameter Server strategy is implemented on a cluster of machines. Some machines will act as the worker and others are parameter servers. Workers compute and parameter servers store the variables\n",
    "\n",
    "ðŸ“œ __NOTE:__ TensorFlow distributed API works better in Graph mode rather than Eager mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
